{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f12e1b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (6.0.0)\n",
      "Requirement already satisfied: sentence-transformers in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (5.1.0)\n",
      "Requirement already satisfied: faiss-cpu in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (1.11.0.post1)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (2.2.6)\n",
      "Requirement already satisfied: openai in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (1.99.9)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from sentence-transformers) (4.55.0)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from sentence-transformers) (1.7.1)\n",
      "Requirement already satisfied: scipy in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from sentence-transformers) (0.34.3)\n",
      "Requirement already satisfied: Pillow in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.7.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.7)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/miniconda3/envs/ds/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf sentence-transformers faiss-cpu numpy openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb8af4",
   "metadata": {},
   "source": [
    "# TEXT ConVERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5799ede6",
   "metadata": {},
   "source": [
    "     pypdf → Reads and extracts text from PDFs.\n",
    "\t•\tsentence-transformers → Creates embeddings (vector representations of text).\n",
    "\t•\tfaiss-cpu → Stores vectors and finds the most similar ones quickly.\n",
    "\t•\tnumpy → Handles numerical arrays (FAISS uses it internally).\n",
    "\t•\topenai → Lets you call GPT models with your API key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd018253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a8d7252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer # Hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceff489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read PDF \n",
    "def read_pdf(path):\n",
    "    reader = PdfReader(path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text() \n",
    "        text += page_text \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab21f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_FILE = \"data/Introduction to Neural Networks.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79a8e153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Introduction to Neural \\nNetworks\\nAgenda - Schedule\\n1. Historical Intro to Neural Nets\\n2. Perceptrons\\n3. Break\\n4. Gradient Descent\\n5. Simple Feed Forward Networks Hyperplanes, bayes theorem, neighbors, why \\nnot just take inspiration from the brain?\\nAgenda - Goals\\n● \\nHistorical/Biological TangentWhile all of these are interesting ideas and could entail years of study, let’s \\nfocus in on the “perceptron” as another attempt at “solving” intelligence. \\n“Can Machines Think?”(1950) - \\nAlan Turing\\nLogic Theorist (1955) - Newell & \\nSimonAutomatons - Ancient \\nEgypt/Greece/China\\nPerceptron Machines (1957) - \\nRosenblatt\\n… \\n1000s of years \\nlater…\\nThe Perceptron - An Imitation of our Neurons\\nIn 1957, Frank Rosenblatt (cognitive researcher/psychologist) \\npublishes a paper titled “The Perceptron: A Probabilistic Model for \\nInformation Storage & Organization in the Brain”\\n“If we are eventually to understand the capability of higher organisms \\nfor perceptual recognition, generalization, recall, and thinking, we must \\nﬁrst have answers to three fundamental questions:\\n1. How is information about the …world sensed…\\n2. In what form is information stored…\\n3. How does information contained in storage…inﬂuence \\nrecognition”\\nThe Perceptron - An Imitation of our Neurons\\nBefore exploring the features of a perceptron, \\nlet’s recognize the most basic components of \\nour brain: the neuron.\\nA neuron is a cell that takes input and provides \\noutput via electrical signals (action potential) \\nonce we cross a certain threshold potential. \\nThis by no means is a biology fellowship, so \\nwe’ll keep it nice and short\\nIn rough terms, the neuron “ﬁres” after it reaches a level of depolarization. Once a neuron reaches around -55 microvolts, it “shoots off” a signal to another neuron in the central/peripheral nervous system.\\n(by no means a neurophysiology lesson) If you bundle enough of these \\nneurons tightly together, you start getting intelligent behavior. For some \\nreason “consciousness” also emerges, whatever that’s about.\\n“Did I turn the stove off?”\\n“What will I get my mom for \\nher birthday?”\\n“2 + 2  = ???”\\nThe Perceptron - An Imitation of our Neurons\\nRosenblatt observed this biological \\nphenomenon (and with the help of past \\nresearch) and posited:\\n“...if one understood the code or ‘wiring diagram’ \\nof the nervous system, one should, in principle, be \\nable to discover exactly what an organism \\nremembers by reconstructing the original \\nsensory pattern…”\\nThe Perceptron - An Imitation of our Neurons\\nWell, lo & behold , a machine that \\ncommunicated via “electric signal” was \\nalready in widespread use.\\nSo why don’t we just try to recreate the \\nbrain on a computer???\\nThe Perceptron - An Imitation of our Neurons\\nAnd so, that’s exactly what \\nRosenblatt did.The Perceptron - An Imitation of our Neurons\\nThe perceptron is a supervised binary \\nlearning classiﬁcation algorithm that takes in \\ninput features, and learned weights.\\nIf these summed weights surpass some \\nactivation function, we classify an output as \\n“1. ” Just like a neuron!\\nThe Perceptron - An Imitation of our Neurons\\nHowever this only worked for linearly \\nseparable classiﬁcations.\\nPerhaps to achieve complex classiﬁcations, we \\ndon’t use one neuron, but rather… \\nThe Perceptron - An Imitation of our Neurons\\nHowever this only worked for linearly \\nseparable classiﬁcations.\\nPerhaps to achieve complex classiﬁcations, we \\ndon’t use one neuron, but rather… \\nA network of neurons (a neural network?)\\nApplications of Neural Networks\\nThere are are many different applications of this \\npowerful concept:\\n● Image recognition via Convolutional Nets\\n● Time series prediction via Long-Short T erm \\nMemory Nets\\n● Embedding generation via Word2Vec\\n● Word prediction via Transformers\\nOur focus will be on these last two concepts\\nApplications of Neural Networks\\nAt least for the time being, almost all novelty (at least the novelty that attracts \\nthe most funding) comes from deep learning models (another word for neural \\nnets).\\nLet’s see a couple of ways in which deep learning methods are being used:Face generation: https://thispersondoesnotexist.com/ \\nNeither of these women are real. Voice generation:  https://elevenlabs.io/ \\nFarukh can’t speak Spanish\\nMore voice generation\\nA Quick Aside - Neurons & Intelligence\\nNeural density is probably not the end-all \\nanswer to intelligence.\\nIf this was the case, elephants & whales \\nwould be “more intelligent” than us. Plus, \\nplenty of life-forms exhibit “intelligence” via \\nother means such as coordination (think of \\nan ant colony).\\nHowever, for carbon-based life-forms like \\nhumans, neural density seems to be a good \\nstart.\\nA Quick Aside - Neurons & Intelligence\\nFurthermore, whether or not something has to be \\nalive to be “intelligent” (or vice-versa) is up for \\ndebate.\\nThink of a virus: a submicroscopic infectious agent \\nthat is not alive, but still exhibits “intelligent” \\nbehavior (mutations).\\nThis topic eventually leads us to topics of \\nconsciousness, which while fun, is not yet applicable \\nto our understanding of these concepts. \\nPerceptrons Review - Classiﬁcation: Binary Categories\\nIn the context of logistic regression (and perceptrons!), we always consider \\na binary classiﬁer to be an “on and off” switch. Is something present or is it \\nnot?\\n● Fraud vs not fraud\\n● Malignant vs nonmalignant \\n● Human vs not human\\n● Dog vs not dog\\nIs this a \\ndog or not  \\na dog?The Perceptron\\nLet’s dive deeper into the internals of a perceptron \\nbefore working through an example.\\nWe have a few components to work with:\\n● Coefﬁcients (W)\\n● Inputs (X)\\nWe express these as vectors which we take the \\ndot-product of.\\nThe Perceptron - A DISCLAIMER\\nNo one uses perceptrons by themselves anymore.\\nIt’s important to KNOW what a perceptron is, and \\nhow it works, but you will never generate just one \\nneuron (unless it's for fun).\\nWe’re all about ‘networks’ of neurons these days,\\nJust to break this notation down a little further. Can anyone ﬁgure out the \\ndot product of W (transpose) & X? Remember, when we take the \\ndot-product we just line up the elements, multiply them and then add them \\nall together. \\nThis is the formula that calculates the “voltage” or output of our perceptron. \\nNext we need to determine a function that “activates” , just our human \\nneurons do.\\nWhich model does this formula look like? \\nHint: think back to linearity. Keep in mind that a neuron only “activates” once we get past a certain \\nthreshold. So therefore, we should probably use a function that behaves \\nsimilarly to a neuron (all or nothing). \\nThere are actually quite a few options when it comes to choosing an \\nactivation function. However, for our single perceptron, we will use the \\nheaviside step function (aka binary step function). \\nThe idea for this function is simple: classify a point as “1” (positive) if the output \\nis greater than or equal to 0, otherwise classify it as “0” (negative).\\nThe basic idea of a perceptron is the following: using learned weights and \\ninputs, calculate a summed value which will either active or fail to activate \\nour neuron. Just like how our biological neurons work! \\nThe Perceptron\\nWe’re only scratching the surface of perceptrons, so \\nlet’s take stock of terms:\\n● Coefﬁcients (W): the weights we are trying to \\nlearn.\\n● Inputs (X): The data itself.\\n● Activation Function: A function to “activate” the \\nneuron. We are currently using the heaviside step \\nfunction as our activation.\\nWhile we have seen the “structure” of a perceptron, how do we actually \\nlearn our weights? Any guesses? Think back to previous strategies.\\nHow do we actually learn these \\nweights?Gradient Descent\\nOne such approach is gradient descent.\\nImagine our solution space is a parabola, we \\npick a random point and calculate the slope \\n(gradient).\\nWe then follow that slope down to the \\nminima.\\nThat local minima tells us the best possible \\nweights for our perceptron! \\nThe Perceptron\\nJust like with LASSO & ridge, we utilize \\ngradient descent to ﬁnd our optimal \\nweights! \\nWe start out with some incorrect weights \\n(typically 0, 0, 0) and iteratively “learn” \\nbetter weights!\\nWe usually aim to provide a visual example before diving into the maths. But \\ntheir truly is no way to discuss a perceptron without maths, so let’s break \\ndown these components. \\nnew weight\\nold weight\\nlearning rate\\ndifference b/w target & output\\nRelevant dimension \\nof data sample Let’s work through a simple example using a dataset. We’ll take a look at \\ncoffee consumed & hours of exercise, as it relates to insomnia.\\nHours of exercise a \\nweek\\nCups of coffee a week Insomnia\\n1 3 1\\n2 4 1\\n3 6 1\\n3 1 0\\n4 3 0\\n1 = Insomnia\\n0 = Good nights rest\\nWhat happens as we \\nincrease \\ncoffee/decrease \\nexercise? We plot these data points on a scatter-plot. Would you state that these are \\nlinearly separable?\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia T o start, we select an “incorrect” vector of weights such as (0, 0, 0). \\nAccording to these weights, what will our weight function look like?\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nThis will give us an equation where all the coefﬁcients are “0” . Next we will plug in all of our samples into this function and calculate their output. What will all these samples calculate?\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\ny1 = 0 + 0(1) + 0(3)\\ny2 = 0 + 0(2) + 0(4)\\ny3 = 0 + 0(3) + 0(6)\\ny4 = 0 + 0(3) + 0(1)\\ny5 = 0 + 0(4) + 0(3)All of these outputs compute to “0. ”\\nAccording to our step-function, what will all of our samples be classiﬁed as?\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\ny1 = 0 + 0(1) + 0(3) = 0\\ny2 = 0 + 0(2) + 0(4) = 0\\ny3 = 0 + 0(3) + 0(6) = 0\\ny4 = 0 + 0(3) + 0(1) = 0\\ny5 = 0 + 0(4) + 0(3) = 0Our heaviside function tells us that this does not meet the threshold for \\n‘activation’ therefore all of these samples get classiﬁed as ‘no insomnia. ’\\nObviously this is an incorrect model. Let’s take note of these misclassiﬁcations…\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\ny1 = 0 + 0(1) + 0(3) = 0\\ny2 = 0 + 0(2) + 0(4) = 0\\ny3 = 0 + 0(3) + 0(6) = 0\\ny4 = 0 + 0(3) + 0(1) = 0\\ny5 = 0 + 0(4) + 0(3) = 0\\nInsomnia (1)\\n Insomnia (1)\\nInsomnia (1)\\nInsomnia (1)\\nInsomnia (1)\\nInsomnia Predicted Insomnia\\n1 1\\n1 1\\n1 1\\n0 1\\n0 1\\nWe can update our weights via gradient descent by following the gradient \\nof these weights! Let’s do one round of this to see how our weights update.\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nInsomnia Predicted Insomnia\\n1 1\\n1 1\\n1 1\\n0 1\\n0 1\\nLet’s start with our w1 weight. Can you ﬁll in the variables of this gradient descent algorithm for the ﬁrst sample?We make use of the previous weight calculation, the errors we’ve made, the value of the data-points themselves, and ﬁnally the learning rate (we’ll use 0.1 in this example)\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nExercise Coffee Insomnia Predicted \\nInsomnia\\n1 3 1 1\\n2 4 1 1\\n3 6 1 1\\n3 1 0 1\\n4 3 0 1\\nw1’ =What will be the value of this new weight?\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nExercise Coffee Insomnia Predicted \\nInsomnia\\n1 3 1 1\\n2 4 1 1\\n3 6 1 1\\n3 1 0 1\\n4 3 0 1\\nw1’ = 0 + (0.1)(1 - 1)(1) Since we look at “weight \\n1” , we only consider \\n“predictor 1” (exercise) of \\nour ﬁrst sampleWe get a value of 0.0! However we are not done yet, we need to update our \\nweights based on ALL SAMPLES (full-batch gradient descent)\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nExercise Coffee Insomnia Predicted \\nInsomnia\\n1 3 1 1\\n2 4 1 1\\n3 6 1 1\\n3 1 0 1\\n4 3 0 1\\nw1’ = 0 + (0.1)(1 - 1)(1) = 0.0You might be thinking, “so do we just reuse ‘0’ as the ‘old weight’ in the next sample calculation?”\\n NO! You actually use the previous weight you calculated!!! Can anyone calculate this new value?\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nExercise Coffee Insomnia Predicted \\nInsomnia\\n1 3 1 1\\n2 4 1 1\\n3 6 1 1\\n3 1 0 1\\n4 3 0 1\\nw1’ = 0 + (0.1)(1 - 1)(1) = 0.0\\nw1’ = 0.0 + (0.1)(1 - 1)(2) And then the cycle continues until we’ve observed all samples! Once we ﬁnish, we get our new weight.\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nExercise Coffee Insomnia Predicted \\nInsomnia\\n1 3 1 1\\n2 4 1 1\\n3 6 1 1\\n3 1 0 1\\n4 3 0 1\\nw1’ = 0 + (0.1)(1 - 1)(1) = 0.0\\nw1’ = 0.0 + (0.1)(1 - 1)(2) = 0.0\\nw1’ = 0.0 + (0.1)(1 - 1)(3) = 0.0\\nw1’ = 0.0 + (0.1)(0 - 1)(3) = -0.3\\nw1’ = -0.3 + (0.1)(0 - 1)(4) = -0.7\\nWhat do you notice when we do (0 - 0)?In more visual terms, we just “followed the gradient” of our errors to create \\na coefﬁcient that is “less wrong. ” It’s not perfect, but it’s a step in the right \\ndirection! We keep doing this until we get to a local minimum. \\nw1 = 0\\nw1 = -0.7\\nw1’ = 0 + (0.1)(1 - 1)(1) = 0.0\\nw1’ = 0.0 + (0.1)(1 - 1)(2) = 0.0\\nw1’ = 0.0 + (0.1)(1 - 1)(3) = 0.0\\nw1’ = 0.0 + (0.1)(0 - 1)(3) = -0.3\\nw1’ = -0.3 + (0.1)(0 - 1)(4) = -0.7An Aside - Full Batch Gradient Descent\\nWhat we just did is called FULL BATCH \\nGRADIENT DESCENT.\\nThis involves going through all data-points \\nand updating our weights.\\nRemember, as technologists we want to be \\n‘cost-effective’ , and full-batch gradient \\ndescent is the opposite of cost-effective \\nwhen you have a bajillion rows.\\nAn Aside - Other Options for Gradient Descent\\nInstead, we utilize either stochastic or \\nmini-batch gradient descent.\\nBoth are more cost-effective and lead to \\napproximately the same optimum.\\nWe will explore these techniques later.\\nEventually after applying these updates for each weight (w0, w1, w2) , we are left with (0.3, -0.7, 1.3)Is this a better classiﬁer???\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\ny1 = 0.3 - 0.7(1) + 1.3(3) = 3.5 → 1\\ny2 = 0.3 - 0.7(2) + 1.3(4) = 4.1 → 1\\ny3 = 0.3 - 0.7(3) + 1.3(6) = 6 → 1\\ny4 = 0.3 - 0.7(3) + 1.3(1) = -0.5 → 0\\ny5 = 0.3 - 0.7(4) + 1.3(3) = 1.4 → 1\\nYes! It seems that we get some nuance to our predictions. Notice that our \\n4th sample correctly gets predicted as “no insomnia”!\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\ny1 = 0.3 - 0.7(1) + 1.3(3) = 3.5 → 1\\ny2 = 0.3 - 0.7(2) + 1.3(4) = 4.1 → 1\\ny3 = 0.3 - 0.7(3) + 1.3(6) = 6 → 1\\ny4 = 0.3 - 0.7(3) + 1.3(1) = -0.5 → 0\\ny5 = 0.3 - 0.7(4) + 1.3(3) = 1.4 → 1\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\ny1 = 0.3 - 0.7(1) + 1.3(3) = 3.5 → 1\\ny2 = 0.3 - 0.7(2) + 1.3(4) = 4.1 → 1\\ny3 = 0.3 - 0.7(3) + 1.3(6) = 6 → 1\\ny4 = 0.3 - 0.7(3) + 1.3(1) = -0.5 → 0\\ny5 = 0.3 - 0.7(4) + 1.3(3) = 1.4 → 1\\nNote this hyperplane is calculated by \\nsetting our equation greater than 0\\nBy solving the inequality b/w x1 & x2 we \\nget the following hyperplane\\nSince this hyperplane still makes a few misclassiﬁcations, we should allow it to keep training via gradient \\ndescent.\\nOnce it reaches a point where the separating hyperplane makes marginal updates, we stop  training. We \\ncontrol this through a hyperparameter called epochs (how many times do we update our weights).\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 1\\nOur previous (w0, w1, w2)The next (w0, w1, w2). Notice that it is now an even better linear separator! \\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 2The third (w0, w1, w2). \\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 3The fourth (w0, w1, w2). \\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 4The ﬁfth (w0, w1, w2). Notice that now we are making marginal \\nimprovements to our weights and we’re basically plateauing out.\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 5Gradient descent ﬁgures it out eventually.\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 6\\nHyperparameter - Epochs\\nEpoch count is a hyperparameter that you can control. \\nHowever we will see that in Keras we can implement a \\ncallback which halts training as soon as we have a \\n“good enough” model.\\nNotice that both train & test accuracy both plateau \\nout after enough epochs!\\nThis means that there is some sweet-spot for epochs \\n(although we might not know where it is).\\nThink of this in terms of the marginal gains in happiness across income. For \\nsome individuals, happiness plateaus at a certain income level. Massive \\ngains in income result in negligible gains in happiness. Our perceptrons \\nbehave the same way, we ﬁnd this plateau by halting training at these \\nmarginal returns.\\nHyperparameter - Learning Rate\\nWe also have learning rate. This controls how \\nwell your gradient descent converges! Notice \\nthat this hyperparam does not plateau, instead:\\nIf your learning rate is too small, gradient descent \\nconverges slowly\\nIf your learning rate is too large, the gradient \\ndescent never converges\\n0.1 is usually a safe bet, but the best hyperparam \\nshould be found via CV .\\nWe’ll see what this looks like for the separating hyperplane, but essentially, \\na large learning rate results in instability, while a small learning rate takes \\nbaby steps.\\nOnly for visual demonstration: the behavior of our separating hyperplane \\nwith TOO LARGE A LEARNING RATE\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 1\\nn = 10Only for visual demonstration: the behavior of our separating hyperplane \\nwith TOO LARGE A LEARNING RATE\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 2\\nn = 10Only for visual demonstration: the behavior of our separating hyperplane \\nwith TOO LARGE A LEARNING RATE\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 3\\nn = 10Only for visual demonstration: the behavior of our separating hyperplane \\nwith TOO LARGE A LEARNING RATE\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 4\\nn = 10Only for visual demonstration: the behavior of our separating hyperplane \\nwith TOO LARGE A LEARNING RATE\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch inﬁnity\\nn = 10\\nThe learning rate is too ambitious. It \\nkeeps overshooting its goal and fails \\neach time.  “T oo enthusiastic” of a learner\\nOnly for visual demonstration: the behavior of our separating hyperplane \\nwith TOO SMALL A LEARNING RATE\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 1\\nn = 0.001Only for visual demonstration: the behavior of our separating hyperplane \\nwith TOO SMALL A LEARNING RATE\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 2\\nn = 0.001Only for visual demonstration: the behavior of our separating hyperplane \\nwith TOO SMALL A LEARNING RATE\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 3\\nn = 0.001Only for visual demonstration: the behavior of our separating hyperplane \\nwith TOO SMALL A LEARNING RATE\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 4\\nn = 0.001Only for visual demonstration: the behavior of our separating hyperplane \\nwith TOO SMALL A LEARNING RATE\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 5\\nn = 0.001Only for visual demonstration: the behavior of our separating hyperplane \\nwith TOO SMALL A LEARNING RATE\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch ~1000000\\nn = 0.001\\nThe learning rate is too shy. It will \\neventually ﬁnd the best hyperplane. \\nBut it will take a long time.“T oo shy” of a learner\\nWhile the “perceptron” is an excellent model for binary classiﬁcation, what \\nare some drawbacks you can see to this model?\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia Just like with maximal margin classiﬁers and logistic regression, our “single layer perceptron” can only model simple (and unrealistic) datasets.We need more power aka more neurons (aka a network of neurons). We will explore this on Wednesday\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\n● Assumes “clean & linear” \\nseparation of data\\n● Only works for binary \\nclasses\\n● No ability to model \\ncomplex patterns \\n● Prone to overﬁt \\nEnd of Class AnnouncementsLab (Due 04/22)\\nYou are a data scientist working at an up & coming music platform startup \\nthat just secured its Series B ﬁnancing. \\nWith this infusion of new cash ﬂow, you are tasked with building an \\nunsupervised recommendation algorithm that will recommend users new \\nsongs based on their previous listening history. As this is a brand new \\nproject, you will have to build this project from scratch.\\nSubmit a link to your GitHub repository by 4/22.\\nTomorrow\\nNeural Networks\\n○ What is a “recurrent” neural \\nnetwork\\n○ How do we build a neural \\nnetwork?\\n○ How do we use neural nets to \\nunderstand sentiment?\\nHow can we re-organize this \\nstructure for better \\npredictions?\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_pdf(PDF_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c3950c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = read_pdf(PDF_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da1651e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 300 # maximum number of characters in each chunk of text.\n",
    "CHUNK_OVERLAP = 50  #each chunk shares 50 characters with the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "848270fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunking function\n",
    "def chunk_text(text, size=300, overlap=50):\n",
    "    return [text[i:i+size] for i in range(0, len(text), size - overlap)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b045d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Introduction to Neural \\nNetworks\\nAgenda - Schedule\\n1. Historical Intro to Neural Nets\\n2. Perceptrons\\n3. Break\\n4. Gradient Descent\\n5. Simple Feed Forward Networks Hyperplanes, bayes theorem, neighbors, why \\nnot just take inspiration from the brain?\\nAgenda - Goals\\n● \\nHistorical/Biological TangentWhile',\n",
       " 'enda - Goals\\n● \\nHistorical/Biological TangentWhile all of these are interesting ideas and could entail years of study, let’s \\nfocus in on the “perceptron” as another attempt at “solving” intelligence. \\n“Can Machines Think?”(1950) - \\nAlan Turing\\nLogic Theorist (1955) - Newell & \\nSimonAutomatons - Anc',\n",
       " ' Theorist (1955) - Newell & \\nSimonAutomatons - Ancient \\nEgypt/Greece/China\\nPerceptron Machines (1957) - \\nRosenblatt\\n… \\n1000s of years \\nlater…\\nThe Perceptron - An Imitation of our Neurons\\nIn 1957, Frank Rosenblatt (cognitive researcher/psychologist) \\npublishes a paper titled “The Perceptron: A Probab',\n",
       " 'publishes a paper titled “The Perceptron: A Probabilistic Model for \\nInformation Storage & Organization in the Brain”\\n“If we are eventually to understand the capability of higher organisms \\nfor perceptual recognition, generalization, recall, and thinking, we must \\nﬁrst have answers to three fundamen',\n",
       " 'king, we must \\nﬁrst have answers to three fundamental questions:\\n1. How is information about the …world sensed…\\n2. In what form is information stored…\\n3. How does information contained in storage…inﬂuence \\nrecognition”\\nThe Perceptron - An Imitation of our Neurons\\nBefore exploring the features of a p',\n",
       " 'f our Neurons\\nBefore exploring the features of a perceptron, \\nlet’s recognize the most basic components of \\nour brain: the neuron.\\nA neuron is a cell that takes input and provides \\noutput via electrical signals (action potential) \\nonce we cross a certain threshold potential. \\nThis by no means is a b',\n",
       " 'tain threshold potential. \\nThis by no means is a biology fellowship, so \\nwe’ll keep it nice and short\\nIn rough terms, the neuron “ﬁres” after it reaches a level of depolarization. Once a neuron reaches around -55 microvolts, it “shoots off” a signal to another neuron in the central/peripheral nervou',\n",
       " 'to another neuron in the central/peripheral nervous system.\\n(by no means a neurophysiology lesson) If you bundle enough of these \\nneurons tightly together, you start getting intelligent behavior. For some \\nreason “consciousness” also emerges, whatever that’s about.\\n“Did I turn the stove off?”\\n“What ',\n",
       " 'r that’s about.\\n“Did I turn the stove off?”\\n“What will I get my mom for \\nher birthday?”\\n“2 + 2  = ???”\\nThe Perceptron - An Imitation of our Neurons\\nRosenblatt observed this biological \\nphenomenon (and with the help of past \\nresearch) and posited:\\n“...if one understood the code or ‘wiring diagram’ \\no',\n",
       " '.if one understood the code or ‘wiring diagram’ \\nof the nervous system, one should, in principle, be \\nable to discover exactly what an organism \\nremembers by reconstructing the original \\nsensory pattern…”\\nThe Perceptron - An Imitation of our Neurons\\nWell, lo & behold , a machine that \\ncommunicated v',\n",
       " 'Well, lo & behold , a machine that \\ncommunicated via “electric signal” was \\nalready in widespread use.\\nSo why don’t we just try to recreate the \\nbrain on a computer???\\nThe Perceptron - An Imitation of our Neurons\\nAnd so, that’s exactly what \\nRosenblatt did.The Perceptron - An Imitation of our Neuron',\n",
       " 'tt did.The Perceptron - An Imitation of our Neurons\\nThe perceptron is a supervised binary \\nlearning classiﬁcation algorithm that takes in \\ninput features, and learned weights.\\nIf these summed weights surpass some \\nactivation function, we classify an output as \\n“1. ” Just like a neuron!\\nThe Perceptro',\n",
       " 'output as \\n“1. ” Just like a neuron!\\nThe Perceptron - An Imitation of our Neurons\\nHowever this only worked for linearly \\nseparable classiﬁcations.\\nPerhaps to achieve complex classiﬁcations, we \\ndon’t use one neuron, but rather… \\nThe Perceptron - An Imitation of our Neurons\\nHowever this only worked f',\n",
       " 'mitation of our Neurons\\nHowever this only worked for linearly \\nseparable classiﬁcations.\\nPerhaps to achieve complex classiﬁcations, we \\ndon’t use one neuron, but rather… \\nA network of neurons (a neural network?)\\nApplications of Neural Networks\\nThere are are many different applications of this \\npower',\n",
       " 'are are many different applications of this \\npowerful concept:\\n● Image recognition via Convolutional Nets\\n● Time series prediction via Long-Short T erm \\nMemory Nets\\n● Embedding generation via Word2Vec\\n● Word prediction via Transformers\\nOur focus will be on these last two concepts\\nApplications of Neu',\n",
       " ' be on these last two concepts\\nApplications of Neural Networks\\nAt least for the time being, almost all novelty (at least the novelty that attracts \\nthe most funding) comes from deep learning models (another word for neural \\nnets).\\nLet’s see a couple of ways in which deep learning methods are being u',\n",
       " 'of ways in which deep learning methods are being used:Face generation: https://thispersondoesnotexist.com/ \\nNeither of these women are real. Voice generation:  https://elevenlabs.io/ \\nFarukh can’t speak Spanish\\nMore voice generation\\nA Quick Aside - Neurons & Intelligence\\nNeural density is probably n',\n",
       " 'eurons & Intelligence\\nNeural density is probably not the end-all \\nanswer to intelligence.\\nIf this was the case, elephants & whales \\nwould be “more intelligent” than us. Plus, \\nplenty of life-forms exhibit “intelligence” via \\nother means such as coordination (think of \\nan ant colony).\\nHowever, for ca',\n",
       " 'ination (think of \\nan ant colony).\\nHowever, for carbon-based life-forms like \\nhumans, neural density seems to be a good \\nstart.\\nA Quick Aside - Neurons & Intelligence\\nFurthermore, whether or not something has to be \\nalive to be “intelligent” (or vice-versa) is up for \\ndebate.\\nThink of a virus: a sub',\n",
       " '-versa) is up for \\ndebate.\\nThink of a virus: a submicroscopic infectious agent \\nthat is not alive, but still exhibits “intelligent” \\nbehavior (mutations).\\nThis topic eventually leads us to topics of \\nconsciousness, which while fun, is not yet applicable \\nto our understanding of these concepts. \\nPerc',\n",
       " 'ble \\nto our understanding of these concepts. \\nPerceptrons Review - Classiﬁcation: Binary Categories\\nIn the context of logistic regression (and perceptrons!), we always consider \\na binary classiﬁer to be an “on and off” switch. Is something present or is it \\nnot?\\n● Fraud vs not fraud\\n● Malignant vs n',\n",
       " ' is it \\nnot?\\n● Fraud vs not fraud\\n● Malignant vs nonmalignant \\n● Human vs not human\\n● Dog vs not dog\\nIs this a \\ndog or not  \\na dog?The Perceptron\\nLet’s dive deeper into the internals of a perceptron \\nbefore working through an example.\\nWe have a few components to work with:\\n● Coefﬁcients (W)\\n● Inputs',\n",
       " 'omponents to work with:\\n● Coefﬁcients (W)\\n● Inputs (X)\\nWe express these as vectors which we take the \\ndot-product of.\\nThe Perceptron - A DISCLAIMER\\nNo one uses perceptrons by themselves anymore.\\nIt’s important to KNOW what a perceptron is, and \\nhow it works, but you will never generate just one \\nneu',\n",
       " \"t works, but you will never generate just one \\nneuron (unless it's for fun).\\nWe’re all about ‘networks’ of neurons these days,\\nJust to break this notation down a little further. Can anyone ﬁgure out the \\ndot product of W (transpose) & X? Remember, when we take the \\ndot-product we just line up the el\",\n",
       " 'en we take the \\ndot-product we just line up the elements, multiply them and then add them \\nall together. \\nThis is the formula that calculates the “voltage” or output of our perceptron. \\nNext we need to determine a function that “activates” , just our human \\nneurons do.\\nWhich model does this formula ',\n",
       " ' human \\nneurons do.\\nWhich model does this formula look like? \\nHint: think back to linearity. Keep in mind that a neuron only “activates” once we get past a certain \\nthreshold. So therefore, we should probably use a function that behaves \\nsimilarly to a neuron (all or nothing). \\nThere are actually qu',\n",
       " ' a neuron (all or nothing). \\nThere are actually quite a few options when it comes to choosing an \\nactivation function. However, for our single perceptron, we will use the \\nheaviside step function (aka binary step function). \\nThe idea for this function is simple: classify a point as “1” (positive) if',\n",
       " 'n is simple: classify a point as “1” (positive) if the output \\nis greater than or equal to 0, otherwise classify it as “0” (negative).\\nThe basic idea of a perceptron is the following: using learned weights and \\ninputs, calculate a summed value which will either active or fail to activate \\nour neuron',\n",
       " 'will either active or fail to activate \\nour neuron. Just like how our biological neurons work! \\nThe Perceptron\\nWe’re only scratching the surface of perceptrons, so \\nlet’s take stock of terms:\\n● Coefﬁcients (W): the weights we are trying to \\nlearn.\\n● Inputs (X): The data itself.\\n● Activation Function',\n",
       " 'Inputs (X): The data itself.\\n● Activation Function: A function to “activate” the \\nneuron. We are currently using the heaviside step \\nfunction as our activation.\\nWhile we have seen the “structure” of a perceptron, how do we actually \\nlearn our weights? Any guesses? Think back to previous strategies.\\n',\n",
       " '? Any guesses? Think back to previous strategies.\\nHow do we actually learn these \\nweights?Gradient Descent\\nOne such approach is gradient descent.\\nImagine our solution space is a parabola, we \\npick a random point and calculate the slope \\n(gradient).\\nWe then follow that slope down to the \\nminima.\\nThat',\n",
       " 'e then follow that slope down to the \\nminima.\\nThat local minima tells us the best possible \\nweights for our perceptron! \\nThe Perceptron\\nJust like with LASSO & ridge, we utilize \\ngradient descent to ﬁnd our optimal \\nweights! \\nWe start out with some incorrect weights \\n(typically 0, 0, 0) and iterative',\n",
       " 'correct weights \\n(typically 0, 0, 0) and iteratively “learn” \\nbetter weights!\\nWe usually aim to provide a visual example before diving into the maths. But \\ntheir truly is no way to discuss a perceptron without maths, so let’s break \\ndown these components. \\nnew weight\\nold weight\\nlearning rate\\ndiffere',\n",
       " 'ents. \\nnew weight\\nold weight\\nlearning rate\\ndifference b/w target & output\\nRelevant dimension \\nof data sample Let’s work through a simple example using a dataset. We’ll take a look at \\ncoffee consumed & hours of exercise, as it relates to insomnia.\\nHours of exercise a \\nweek\\nCups of coffee a week Inso',\n",
       " 'urs of exercise a \\nweek\\nCups of coffee a week Insomnia\\n1 3 1\\n2 4 1\\n3 6 1\\n3 1 0\\n4 3 0\\n1 = Insomnia\\n0 = Good nights rest\\nWhat happens as we \\nincrease \\ncoffee/decrease \\nexercise? We plot these data points on a scatter-plot. Would you state that these are \\nlinearly separable?\\nHours Exercise a week\\nCups ',\n",
       " 'e \\nlinearly separable?\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia T o start, we select an “incorrect” vector of weights such as (0, 0, 0). \\nAccording to these weights, what will our weight function look like?\\nHours Exerci',\n",
       " 't will our weight function look like?\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nThis will give us an equation where all the coefﬁcients are “0” . Next we will plug in all of our samples into this function and calculate ',\n",
       " 'l of our samples into this function and calculate their output. What will all these samples calculate?\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\ny1 = 0 + 0(1) + 0(3)\\ny2 = 0 + 0(2) + 0(4)\\ny3 = 0 + 0(3) + 0(6)\\ny4 = 0 + 0(',\n",
       " '= 0 + 0(2) + 0(4)\\ny3 = 0 + 0(3) + 0(6)\\ny4 = 0 + 0(3) + 0(1)\\ny5 = 0 + 0(4) + 0(3)All of these outputs compute to “0. ”\\nAccording to our step-function, what will all of our samples be classiﬁed as?\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n     ',\n",
       " '5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\ny1 = 0 + 0(1) + 0(3) = 0\\ny2 = 0 + 0(2) + 0(4) = 0\\ny3 = 0 + 0(3) + 0(6) = 0\\ny4 = 0 + 0(3) + 0(1) = 0\\ny5 = 0 + 0(4) + 0(3) = 0Our heaviside function tells us that this does not meet the threshold for \\n‘activation’ therefore all of th',\n",
       " 'he threshold for \\n‘activation’ therefore all of these samples get classiﬁed as ‘no insomnia. ’\\nObviously this is an incorrect model. Let’s take note of these misclassiﬁcations…\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\n',\n",
       " ' 8 9\\n         = Insomnia \\n         = No Insomnia \\ny1 = 0 + 0(1) + 0(3) = 0\\ny2 = 0 + 0(2) + 0(4) = 0\\ny3 = 0 + 0(3) + 0(6) = 0\\ny4 = 0 + 0(3) + 0(1) = 0\\ny5 = 0 + 0(4) + 0(3) = 0\\nInsomnia (1)\\n Insomnia (1)\\nInsomnia (1)\\nInsomnia (1)\\nInsomnia (1)\\nInsomnia Predicted Insomnia\\n1 1\\n1 1\\n1 1\\n0 1\\n0 1\\nWe can upda',\n",
       " 'Predicted Insomnia\\n1 1\\n1 1\\n1 1\\n0 1\\n0 1\\nWe can update our weights via gradient descent by following the gradient \\nof these weights! Let’s do one round of this to see how our weights update.\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = N',\n",
       " '\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nInsomnia Predicted Insomnia\\n1 1\\n1 1\\n1 1\\n0 1\\n0 1\\nLet’s start with our w1 weight. Can you ﬁll in the variables of this gradient descent algorithm for the ﬁrst sample?We make use of the previous weight calculation, the errors we’ve made, the',\n",
       " 'ous weight calculation, the errors we’ve made, the value of the data-points themselves, and ﬁnally the learning rate (we’ll use 0.1 in this example)\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nExercise Coffee Insomnia Pre',\n",
       " '       = No Insomnia \\nExercise Coffee Insomnia Predicted \\nInsomnia\\n1 3 1 1\\n2 4 1 1\\n3 6 1 1\\n3 1 0 1\\n4 3 0 1\\nw1’ =What will be the value of this new weight?\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nExercise Coffee Insomn',\n",
       " 'ia \\n         = No Insomnia \\nExercise Coffee Insomnia Predicted \\nInsomnia\\n1 3 1 1\\n2 4 1 1\\n3 6 1 1\\n3 1 0 1\\n4 3 0 1\\nw1’ = 0 + (0.1)(1 - 1)(1) Since we look at “weight \\n1” , we only consider \\n“predictor 1” (exercise) of \\nour ﬁrst sampleWe get a value of 0.0! However we are not done yet, we need to updat',\n",
       " '0.0! However we are not done yet, we need to update our \\nweights based on ALL SAMPLES (full-batch gradient descent)\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nExercise Coffee Insomnia Predicted \\nInsomnia\\n1 3 1 1\\n2 4 1 1\\n',\n",
       " 'ffee Insomnia Predicted \\nInsomnia\\n1 3 1 1\\n2 4 1 1\\n3 6 1 1\\n3 1 0 1\\n4 3 0 1\\nw1’ = 0 + (0.1)(1 - 1)(1) = 0.0You might be thinking, “so do we just reuse ‘0’ as the ‘old weight’ in the next sample calculation?”\\n NO! You actually use the previous weight you calculated!!! Can anyone calculate this new valu',\n",
       " 'u calculated!!! Can anyone calculate this new value?\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nExercise Coffee Insomnia Predicted \\nInsomnia\\n1 3 1 1\\n2 4 1 1\\n3 6 1 1\\n3 1 0 1\\n4 3 0 1\\nw1’ = 0 + (0.1)(1 - 1)(1) = 0.0\\nw1’ = 0',\n",
       " ' 1\\n4 3 0 1\\nw1’ = 0 + (0.1)(1 - 1)(1) = 0.0\\nw1’ = 0.0 + (0.1)(1 - 1)(2) And then the cycle continues until we’ve observed all samples! Once we ﬁnish, we get our new weight.\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nExerc',\n",
       " '         = Insomnia \\n         = No Insomnia \\nExercise Coffee Insomnia Predicted \\nInsomnia\\n1 3 1 1\\n2 4 1 1\\n3 6 1 1\\n3 1 0 1\\n4 3 0 1\\nw1’ = 0 + (0.1)(1 - 1)(1) = 0.0\\nw1’ = 0.0 + (0.1)(1 - 1)(2) = 0.0\\nw1’ = 0.0 + (0.1)(1 - 1)(3) = 0.0\\nw1’ = 0.0 + (0.1)(0 - 1)(3) = -0.3\\nw1’ = -0.3 + (0.1)(0 - 1)(4) = -0.7',\n",
       " '- 1)(3) = -0.3\\nw1’ = -0.3 + (0.1)(0 - 1)(4) = -0.7\\nWhat do you notice when we do (0 - 0)?In more visual terms, we just “followed the gradient” of our errors to create \\na coefﬁcient that is “less wrong. ” It’s not perfect, but it’s a step in the right \\ndirection! We keep doing this until we get to a ',\n",
       " ' \\ndirection! We keep doing this until we get to a local minimum. \\nw1 = 0\\nw1 = -0.7\\nw1’ = 0 + (0.1)(1 - 1)(1) = 0.0\\nw1’ = 0.0 + (0.1)(1 - 1)(2) = 0.0\\nw1’ = 0.0 + (0.1)(1 - 1)(3) = 0.0\\nw1’ = 0.0 + (0.1)(0 - 1)(3) = -0.3\\nw1’ = -0.3 + (0.1)(0 - 1)(4) = -0.7An Aside - Full Batch Gradient Descent\\nWhat we ',\n",
       " '0.7An Aside - Full Batch Gradient Descent\\nWhat we just did is called FULL BATCH \\nGRADIENT DESCENT.\\nThis involves going through all data-points \\nand updating our weights.\\nRemember, as technologists we want to be \\n‘cost-effective’ , and full-batch gradient \\ndescent is the opposite of cost-effective \\nw',\n",
       " 'ient \\ndescent is the opposite of cost-effective \\nwhen you have a bajillion rows.\\nAn Aside - Other Options for Gradient Descent\\nInstead, we utilize either stochastic or \\nmini-batch gradient descent.\\nBoth are more cost-effective and lead to \\napproximately the same optimum.\\nWe will explore these techni',\n",
       " 'ely the same optimum.\\nWe will explore these techniques later.\\nEventually after applying these updates for each weight (w0, w1, w2) , we are left with (0.3, -0.7, 1.3)Is this a better classiﬁer???\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n     ',\n",
       " '5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\ny1 = 0.3 - 0.7(1) + 1.3(3) = 3.5 → 1\\ny2 = 0.3 - 0.7(2) + 1.3(4) = 4.1 → 1\\ny3 = 0.3 - 0.7(3) + 1.3(6) = 6 → 1\\ny4 = 0.3 - 0.7(3) + 1.3(1) = -0.5 → 0\\ny5 = 0.3 - 0.7(4) + 1.3(3) = 1.4 → 1\\nYes! It seems that we get some nuance to our pr',\n",
       " ' 1\\nYes! It seems that we get some nuance to our predictions. Notice that our \\n4th sample correctly gets predicted as “no insomnia”!\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\ny1 = 0.3 - 0.7(1) + 1.3(3) = 3.5 → 1\\ny2 = 0.3',\n",
       " 'nia \\ny1 = 0.3 - 0.7(1) + 1.3(3) = 3.5 → 1\\ny2 = 0.3 - 0.7(2) + 1.3(4) = 4.1 → 1\\ny3 = 0.3 - 0.7(3) + 1.3(6) = 6 → 1\\ny4 = 0.3 - 0.7(3) + 1.3(1) = -0.5 → 0\\ny5 = 0.3 - 0.7(4) + 1.3(3) = 1.4 → 1\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = N',\n",
       " '\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\ny1 = 0.3 - 0.7(1) + 1.3(3) = 3.5 → 1\\ny2 = 0.3 - 0.7(2) + 1.3(4) = 4.1 → 1\\ny3 = 0.3 - 0.7(3) + 1.3(6) = 6 → 1\\ny4 = 0.3 - 0.7(3) + 1.3(1) = -0.5 → 0\\ny5 = 0.3 - 0.7(4) + 1.3(3) = 1.4 → 1\\nNote this hyperplane is calculated by \\nsetting our equ',\n",
       " ' this hyperplane is calculated by \\nsetting our equation greater than 0\\nBy solving the inequality b/w x1 & x2 we \\nget the following hyperplane\\nSince this hyperplane still makes a few misclassiﬁcations, we should allow it to keep training via gradient \\ndescent.\\nOnce it reaches a point where the separa',\n",
       " '\\ndescent.\\nOnce it reaches a point where the separating hyperplane makes marginal updates, we stop  training. We \\ncontrol this through a hyperparameter called epochs (how many times do we update our weights).\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Ins',\n",
       " 'eek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 1\\nOur previous (w0, w1, w2)The next (w0, w1, w2). Notice that it is now an even better lin',\n",
       " ' w1, w2). Notice that it is now an even better linear separator! \\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 2The third (w0, w1, w2). \\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n ',\n",
       " ' \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 3The fourth (w0, w1, w2). \\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 4The ﬁfth (w0, w1, w2). Notice that now we a',\n",
       " 'Epoch 4The ﬁfth (w0, w1, w2). Notice that now we are making marginal \\nimprovements to our weights and we’re basically plateauing out.\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 5Gradient descent ﬁgures it out event',\n",
       " 'omnia \\nEpoch 5Gradient descent ﬁgures it out eventually.\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 6\\nHyperparameter - Epochs\\nEpoch count is a hyperparameter that you can control. \\nHowever we will see that in Keras',\n",
       " 'ou can control. \\nHowever we will see that in Keras we can implement a \\ncallback which halts training as soon as we have a \\n“good enough” model.\\nNotice that both train & test accuracy both plateau \\nout after enough epochs!\\nThis means that there is some sweet-spot for epochs \\n(although we might not kn',\n",
       " 'e sweet-spot for epochs \\n(although we might not know where it is).\\nThink of this in terms of the marginal gains in happiness across income. For \\nsome individuals, happiness plateaus at a certain income level. Massive \\ngains in income result in negligible gains in happiness. Our perceptrons \\nbehave t',\n",
       " 'ible gains in happiness. Our perceptrons \\nbehave the same way, we ﬁnd this plateau by halting training at these \\nmarginal returns.\\nHyperparameter - Learning Rate\\nWe also have learning rate. This controls how \\nwell your gradient descent converges! Notice \\nthat this hyperparam does not plateau, instea',\n",
       " 'ice \\nthat this hyperparam does not plateau, instead:\\nIf your learning rate is too small, gradient descent \\nconverges slowly\\nIf your learning rate is too large, the gradient \\ndescent never converges\\n0.1 is usually a safe bet, but the best hyperparam \\nshould be found via CV .\\nWe’ll see what this looks',\n",
       " 'should be found via CV .\\nWe’ll see what this looks like for the separating hyperplane, but essentially, \\na large learning rate results in instability, while a small learning rate takes \\nbaby steps.\\nOnly for visual demonstration: the behavior of our separating hyperplane \\nwith TOO LARGE A LEARNING RA',\n",
       " 'eparating hyperplane \\nwith TOO LARGE A LEARNING RATE\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 1\\nn = 10Only for visual demonstration: the behavior of our separating hyperplane \\nwith TOO LARGE A LEARNING RATE\\nHours',\n",
       " 'g hyperplane \\nwith TOO LARGE A LEARNING RATE\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 2\\nn = 10Only for visual demonstration: the behavior of our separating hyperplane \\nwith TOO LARGE A LEARNING RATE\\nHours Exercis',\n",
       " 'lane \\nwith TOO LARGE A LEARNING RATE\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 3\\nn = 10Only for visual demonstration: the behavior of our separating hyperplane \\nwith TOO LARGE A LEARNING RATE\\nHours Exercise a week',\n",
       " 'th TOO LARGE A LEARNING RATE\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 4\\nn = 10Only for visual demonstration: the behavior of our separating hyperplane \\nwith TOO LARGE A LEARNING RATE\\nHours Exercise a week\\nCups of',\n",
       " 'ARGE A LEARNING RATE\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch inﬁnity\\nn = 10\\nThe learning rate is too ambitious. It \\nkeeps overshooting its goal and fails \\neach time.  “T oo enthusiastic” of a learner\\nOnly for vi',\n",
       " 'ime.  “T oo enthusiastic” of a learner\\nOnly for visual demonstration: the behavior of our separating hyperplane \\nwith TOO SMALL A LEARNING RATE\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 1\\nn = 0.001Only for visual ',\n",
       " '  = No Insomnia \\nEpoch 1\\nn = 0.001Only for visual demonstration: the behavior of our separating hyperplane \\nwith TOO SMALL A LEARNING RATE\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 2\\nn = 0.001Only for visual demon',\n",
       " 'o Insomnia \\nEpoch 2\\nn = 0.001Only for visual demonstration: the behavior of our separating hyperplane \\nwith TOO SMALL A LEARNING RATE\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 3\\nn = 0.001Only for visual demonstrat',\n",
       " 'omnia \\nEpoch 3\\nn = 0.001Only for visual demonstration: the behavior of our separating hyperplane \\nwith TOO SMALL A LEARNING RATE\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 4\\nn = 0.001Only for visual demonstration: ',\n",
       " ' \\nEpoch 4\\nn = 0.001Only for visual demonstration: the behavior of our separating hyperplane \\nwith TOO SMALL A LEARNING RATE\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch 5\\nn = 0.001Only for visual demonstration: the b',\n",
       " 'ch 5\\nn = 0.001Only for visual demonstration: the behavior of our separating hyperplane \\nwith TOO SMALL A LEARNING RATE\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\nEpoch ~1000000\\nn = 0.001\\nThe learning rate is too shy. It ',\n",
       " '000000\\nn = 0.001\\nThe learning rate is too shy. It will \\neventually ﬁnd the best hyperplane. \\nBut it will take a long time.“T oo shy” of a learner\\nWhile the “perceptron” is an excellent model for binary classiﬁcation, what \\nare some drawbacks you can see to this model?\\nHours Exercise a week\\nCups of \\n',\n",
       " 'see to this model?\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia Just like with maximal margin classiﬁers and logistic regression, our “single layer perceptron” can only model simple (and unrealistic) datasets.We need more p',\n",
       " 'l simple (and unrealistic) datasets.We need more power aka more neurons (aka a network of neurons). We will explore this on Wednesday\\nHours Exercise a week\\nCups of \\nCoffee a \\nWeek\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n42 3 51 6 7 8 9\\n         = Insomnia \\n         = No Insomnia \\n● Assumes “clean & linear” \\nseparation of d',\n",
       " 'omnia \\n● Assumes “clean & linear” \\nseparation of data\\n● Only works for binary \\nclasses\\n● No ability to model \\ncomplex patterns \\n● Prone to overﬁt \\nEnd of Class AnnouncementsLab (Due 04/22)\\nYou are a data scientist working at an up & coming music platform startup \\nthat just secured its Series B ﬁnanc',\n",
       " 'form startup \\nthat just secured its Series B ﬁnancing. \\nWith this infusion of new cash ﬂow, you are tasked with building an \\nunsupervised recommendation algorithm that will recommend users new \\nsongs based on their previous listening history. As this is a brand new \\nproject, you will have to build t',\n",
       " ' is a brand new \\nproject, you will have to build this project from scratch.\\nSubmit a link to your GitHub repository by 4/22.\\nTomorrow\\nNeural Networks\\n○ What is a “recurrent” neural \\nnetwork\\n○ How do we build a neural \\nnetwork?\\n○ How do we use neural nets to \\nunderstand sentiment?\\nHow can we re-organ',\n",
       " 'nets to \\nunderstand sentiment?\\nHow can we re-organize this \\nstructure for better \\npredictions?\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05bf3580",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ef6c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\" #embedding model used to turn text into vectors.\n",
    "\t# \"all-MiniLM-L6-v2\" is a fast, small, accurate model from Sentence Transformers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3fb0d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "embedder = SentenceTransformer(EMBED_MODEL) # hugging face model\n",
    "embeddings = embedder.encode(chunks, normalize_embeddings=True)\n",
    "embeddings = np.array(embeddings, dtype=\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dc7a13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 384)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape # 108 chunks and 384 dim- “meaning coordinates” for that text in a semantic space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45a5b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Store in FAISS index\n",
    "dim = embeddings.shape[1] # 384 dim of each vector, this model only gives you 384 dim\n",
    "index = faiss.IndexFlatIP(dim) # Creates a FAISS index object that will store and search your vectors.\n",
    "index.add(embeddings) # add my vectors (document chunks) into faiss(store vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9578fe57",
   "metadata": {},
   "source": [
    "This is my document chunk vectors (each 384-dim long). Store them so FAISS when I give you a query vector later, you can quickly find the most similar ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da013310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(index.ntotal) # vectors stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54184a28",
   "metadata": {},
   "source": [
    "How it works in RAG\n",
    "- 1.\tRetriever (Hugging Face model + FAISS) → Finds relevant chunks.\n",
    "- 2.\tLLM (OpenAI GPT, or any other LLM) → Reads those chunks + your question → writes a final, well-formed answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fb7c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG \n",
    "OPENAI_API_KEY = \"\"\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "client = OpenAI(api_key= OPENAI_API_KEY\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87c871e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = 3 #FAISS searches all chunks and returns top 3 most similar ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b17beed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    # embed the query (same model as documents)\n",
    "    ques_emb = embedder.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
    "\n",
    "    # retrieve top-k most similar chunks from FAISS\n",
    "    D, I = index.search(ques_emb, TOP_K)     # D: scores, I: indices into chunks list\n",
    "    scores = D[0].tolist()\n",
    "    idxs = I[0].tolist()\n",
    "\n",
    "\n",
    "    print(\"\\n--- Chunks-------\")\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        if idx == -1: \n",
    "            continue\n",
    "        print(f\"[score={score:.3f} idx={idx}] {chunks[idx][:200]}...\")\n",
    "\n",
    "    # join chunks --{'\\n\\n'.join(retrieved_chunks)\n",
    "    \n",
    "    context = \"\\n\\n\".join(                 #  join everything together, separated by 2 newlines\n",
    "    chunks[idx]                        # take the text chunk at this index\n",
    "    for idx in I[0]                     # loop through each index in the first result row from FAISS\n",
    "    if idx != -1  )\n",
    "\n",
    "    print(f'--------Context-----------{context}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a88ea96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunks-------\n",
      "[score=0.643 idx=5] f our Neurons\n",
      "Before exploring the features of a perceptron, \n",
      "let’s recognize the most basic components of \n",
      "our brain: the neuron.\n",
      "A neuron is a cell that takes input and provides \n",
      "output via electric...\n",
      "[score=0.548 idx=12] output as \n",
      "“1. ” Just like a neuron!\n",
      "The Perceptron - An Imitation of our Neurons\n",
      "However this only worked for linearly \n",
      "separable classiﬁcations.\n",
      "Perhaps to achieve complex classiﬁcations, we \n",
      "don’t ...\n",
      "[score=0.542 idx=13] mitation of our Neurons\n",
      "However this only worked for linearly \n",
      "separable classiﬁcations.\n",
      "Perhaps to achieve complex classiﬁcations, we \n",
      "don’t use one neuron, but rather… \n",
      "A network of neurons (a neura...\n",
      "--------Context-----------f our Neurons\n",
      "Before exploring the features of a perceptron, \n",
      "let’s recognize the most basic components of \n",
      "our brain: the neuron.\n",
      "A neuron is a cell that takes input and provides \n",
      "output via electrical signals (action potential) \n",
      "once we cross a certain threshold potential. \n",
      "This by no means is a b\n",
      "\n",
      "output as \n",
      "“1. ” Just like a neuron!\n",
      "The Perceptron - An Imitation of our Neurons\n",
      "However this only worked for linearly \n",
      "separable classiﬁcations.\n",
      "Perhaps to achieve complex classiﬁcations, we \n",
      "don’t use one neuron, but rather… \n",
      "The Perceptron - An Imitation of our Neurons\n",
      "However this only worked f\n",
      "\n",
      "mitation of our Neurons\n",
      "However this only worked for linearly \n",
      "separable classiﬁcations.\n",
      "Perhaps to achieve complex classiﬁcations, we \n",
      "don’t use one neuron, but rather… \n",
      "A network of neurons (a neural network?)\n",
      "Applications of Neural Networks\n",
      "There are are many different applications of this \n",
      "power\n"
     ]
    }
   ],
   "source": [
    "rag('what is a neuron?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da5c98b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  query embedding\n",
    "def rag_answer(query):\n",
    "    q_emb = embedder.encode([query], normalize_embeddings=True).astype(\"float32\") # query vector\n",
    "    D, I = index.search(q_emb, TOP_K) # compare query vectortop  and 3 most similar chunks.\n",
    "\n",
    "    # Get top chunks\n",
    "    retrieved_chunks = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        retrieved_chunks.append(f\"[Source chunk {idx}] {chunks[idx]}\")\n",
    "\n",
    "        # GPT prompt\n",
    "    prompt = f\"\"\"\n",
    "       You are a helpful assistant. Use the provided context to answer the question.\n",
    "\n",
    "      Context:\n",
    "      {'\\n'.join(retrieved_chunks)}\n",
    "\n",
    "      Question: {query}\n",
    "      Answer with facts from the context. If unsure, say 'I don't know'.\n",
    "     \n",
    "       \"\"\"\n",
    "\n",
    "    # GPT call\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',  \n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content, retrieved_chunks\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6540071b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A neuron is a cell that takes input and provides output via electrical signals (action potential) once a certain threshold potential is crossed.',\n",
       " ['[Source chunk 5] f our Neurons\\nBefore exploring the features of a perceptron, \\nlet’s recognize the most basic components of \\nour brain: the neuron.\\nA neuron is a cell that takes input and provides \\noutput via electrical signals (action potential) \\nonce we cross a certain threshold potential. \\nThis by no means is a b',\n",
       "  '[Source chunk 13] mitation of our Neurons\\nHowever this only worked for linearly \\nseparable classiﬁcations.\\nPerhaps to achieve complex classiﬁcations, we \\ndon’t use one neuron, but rather… \\nA network of neurons (a neural network?)\\nApplications of Neural Networks\\nThere are are many different applications of this \\npower',\n",
       "  '[Source chunk 12] output as \\n“1. ” Just like a neuron!\\nThe Perceptron - An Imitation of our Neurons\\nHowever this only worked for linearly \\nseparable classiﬁcations.\\nPerhaps to achieve complex classiﬁcations, we \\ndon’t use one neuron, but rather… \\nThe Perceptron - An Imitation of our Neurons\\nHowever this only worked f'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_answer(\"what is a neuron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c2b954",
   "metadata": {},
   "source": [
    "________________END_____________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cfc3fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"I don't know.\",\n",
       " ['[Source chunk 12] output as \\n“1. ” Just like a neuron!\\nThe Perceptron - An Imitation of our Neurons\\nHowever this only worked for linearly \\nseparable classiﬁcations.\\nPerhaps to achieve complex classiﬁcations, we \\ndon’t use one neuron, but rather… \\nThe Perceptron - An Imitation of our Neurons\\nHowever this only worked f',\n",
       "  '[Source chunk 8] r that’s about.\\n“Did I turn the stove off?”\\n“What will I get my mom for \\nher birthday?”\\n“2 + 2  = ???”\\nThe Perceptron - An Imitation of our Neurons\\nRosenblatt observed this biological \\nphenomenon (and with the help of past \\nresearch) and posited:\\n“...if one understood the code or ‘wiring diagram’ \\no',\n",
       "  '[Source chunk 4] king, we must \\nﬁrst have answers to three fundamental questions:\\n1. How is information about the …world sensed…\\n2. In what form is information stored…\\n3. How does information contained in storage…inﬂuence \\nrecognition”\\nThe Perceptron - An Imitation of our Neurons\\nBefore exploring the features of a p'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_answer('What is z?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bed4461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"I don't know.\",\n",
       " ['[Source chunk 89]  is a brand new \\nproject, you will have to build this project from scratch.\\nSubmit a link to your GitHub repository by 4/22.\\nTomorrow\\nNeural Networks\\n○ What is a “recurrent” neural \\nnetwork\\n○ How do we build a neural \\nnetwork?\\n○ How do we use neural nets to \\nunderstand sentiment?\\nHow can we re-organ',\n",
       "  '[Source chunk 0] Introduction to Neural \\nNetworks\\nAgenda - Schedule\\n1. Historical Intro to Neural Nets\\n2. Perceptrons\\n3. Break\\n4. Gradient Descent\\n5. Simple Feed Forward Networks Hyperplanes, bayes theorem, neighbors, why \\nnot just take inspiration from the brain?\\nAgenda - Goals\\n● \\nHistorical/Biological TangentWhile',\n",
       "  '[Source chunk 9] .if one understood the code or ‘wiring diagram’ \\nof the nervous system, one should, in principle, be \\nable to discover exactly what an organism \\nremembers by reconstructing the original \\nsensory pattern…”\\nThe Perceptron - An Imitation of our Neurons\\nWell, lo & behold , a machine that \\ncommunicated v'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_answer('Can you show me a deep neural network diagram?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
